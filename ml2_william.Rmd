---
title: "Computer lab 2 block 1"
author:
- Simge Cinar
- Duc Tran
- William Wiik
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  pdf_document:
    fig_caption: yes
    number_sections: yes
  word_document: default
  html_document:
    df_print: paged
geometry: top=100pt,bottom=100pt,left=68pt,right=66pt
subtitle: 732A99
header-includes:
- \usepackage{booktabs}
- \usepackage{float}
- \usepackage{longtable}
- \usepackage{caption}
- \usepackage{fancyhdr}
- \usepackage{titling}
- \usepackage[swedish, english]{babel}
- \renewcommand{\headrulewidth}{0pt}
- \renewcommand{\and}{\\}
- \pretitle{\centering\vspace{0cm}{\large Laboration report in Machine Learning
  \par}\vspace{4cm}\Huge\textbf}
- \posttitle{\vspace{1cm}\large\textbf{}\par}
- \preauthor{\centering\vspace{4cm}\normalsize}
- \postauthor{\par\vspace{3cm}}
- \predate{\centering{\normalsize Division of Statistics and Machine Learning \\ Department
  of Computer Science \\ Linköping University \par}}
- \postdate{\par\vspace{2cm}}
- \raggedbottom
---

<!-- <!-- Väljer språk till svenska för automatiska titlar -->
<!-- \selectlanguage{swedish} -->

<!-- Byter språket på figur- och tabellbeskrivningar till angivna namn -->
\captionsetup[table]{name = Table}


<!-- Anger sidnumreringens position -->
\fancyhf{}
\fancyfoot[C]{\thepage}
\pagestyle{fancy}

<!-- Tar bort sidnumrering för förteckningar och titelsidan -->
\pagenumbering{gobble}

<!-- Anger sidbrytning -->
\clearpage

<!-- Skapar en innehållsförteckning och anger djupet av rubrikerna som ska visas -->
\setcounter{tocdepth}{3}
\tableofcontents

<!-- Anger sidbrytning -->
\clearpage

<!-- Börjar sidnumreringen på sida 1 efter att alla förteckningar visats -->
\pagenumbering{arabic}
\setcounter{page}{1}

<!-- Börjar med kapitel 1 -->

```{r options, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
library(kknn)
library(dplyr)
library(knitr)
library(caret)
library(psych) 
knitr::opts_chunk$set(
  echo = TRUE, 
  fig.width = 4.5, 
  fig.height = 3)
```



# Assignment 1. Explicit regularization


The **tecator.csv** contains the results of study aimed to investigate whether a near
infrared absorbance spectrum can be used to predict the fat content of samples of
meat. For each meat sample the data consists of a 100 channel spectrum of
absorbance records and the levels of moisture (water), fat and protein. The
absorbance is -log10 of the transmittance measured by the spectrometer. The
moisture, fat and protein are determined by analytic chemistry.
Divide data randomly into train and test (50/50) by using the codes from the
lectures.

```{r}
set.seed(12345) 


# Read in data
tecator <- read.csv("tecator.csv")


# Partitioning training data (50%)
n <- dim(tecator)[1]
id <- sample(1:n, floor(n*0.5)) 
tecator_train <- tecator[id,] 


# Partitioning test data (50%)
tecator_test <- tecator[-id,]


```

##  Question 1.1

**Question:**

Assume that Fat can be modeled as a linear regression in which absorbance characteristics (Channels) are used as features. Report the underlying probabilistic model, fit the linear regression to the training data and estimate the training and test errors. 

**Answer:** 

Probabilistic model:

$$\hat{Fat} =\hat{ \theta_0} + \hat{\theta}_1x_{1}+...+\hat{\theta}_{100}x_{100} + \epsilon$$



```{r}

# column 1, 103 and 104 are sample, moisture and protein and should
# not be used as features
lm_model <- lm(Fat ~ ., data = tecator_train[,-c(1,103:104)])



pred_train <- predict(lm_model, newdata = tecator_train)
pred_test <- predict(lm_model, newdata = tecator_test)

mse_train <- mean((tecator_train$Fat - pred_train)^2)
mse_test <- mean((tecator_test$Fat - pred_test)^2)

```

```{r}

df <- data.frame("MSE" = c(mse_train, mse_test))
rownames(df) <- c("Train","Test")

knitr::kable(df, row.names = TRUE, caption = "Training and test errors", digits = 4)

```


**Question:**

Comment on the quality of fit and prediction and therefore on the quality of model.

**Answer:** 

The MSE for the training data is very low, 0.0057 and for the test data the MSE is high, approximately
722. This means that the model is very overfitted from the training data and therefor the quality for the model is bad. This is expected, due to $n$  (107) is almost equal to $p$ (100).



##  Question 1.2

**Question:**

Assume now that Fat can be modeled as a LASSO regression in which all Channels are used as features. Report the cost function that should be optimized in this scenario. 

**Answer:** 


Cost function:

$$\hat{\theta}^{lasso} = argmin \left\{ \frac{1}{n} \sum_{i=1}^n(y_i - \theta_0 - \theta_1x_{1i}-...-\theta_{100}x_{100i})^2 + \lambda + \sum_{j=1}^p|\theta_j| \right\}$$

* $n$ is the number of observations

* $y_i$ is the observed value for the $i$-th observation.

* $x_{ij}$ is the $j$-th feature (channel) value the $i$-th observation.

* $\theta_0$ is the intercept term.

* $\theta_j$ is the regression coefficient for the $j$-th feature

* $\lambda > 0$ is penalty factor




##  Question 1.3

**Question:**

Fit the LASSO regression model to the training data. Present a plot illustrating how the regression coefficients depend on the log of penalty factor (log $\lambda$) and interpret this plot. What value of the penalty factor can be chosen if we want to select a model with only three features?


**Answer:** 


```{r}
library(caret)
library(glmnet)

set.seed(12345) 


scaler <- preProcess(tecator_train)
data1 <- predict(scaler, tecator_train)
covariates <- data1[,2:101]
response <- data1[, 102]

model_lasso <- glmnet(as.matrix(covariates),response, alpha = 1,
                      family="gaussian")

plot(model_lasso, xvar="lambda", label=TRUE)

# print(model_lasso)
n_features <- 3
index <- which(model_lasso$df == (n_features + 1))[1]
value_penalty_factor <- model_lasso$lambda[index]


coef(model_lasso, s = value_penalty_factor)


```

In figure X, the penalty factor $log(\lambda)$ and the coefficients for the features are plotted.
As $log(\lambda)$ gets higher the more coefficients reach to zero.
If the penalty factor $log(\lambda)$ is higher than -2.5, then the model select only one feature.
If we want to select a model with only three features, then we would have a $log(\lambda)$ equal to 



##  Question 1.4

**Question:**

Repeat step 3 but fit Ridge instead of the LASSO regression and compare the plots from steps 3 and 4. Conclusions?


**Answer:** 


```{r}


model_ridge <- glmnet(as.matrix(covariates), response, alpha = 0,
                      family = "gaussian")

plot(model_ridge, xvar="lambda", label=TRUE)

```




##  Question 1.5

**Question:**

Use cross-validation with default number of folds to compute the optimal LASSO model. Present a plot showing the dependence of the CV score on log $\lambda$ and comment how the CV score changes with log $\lambda$. Report the optimal $\lambda$ and how many variables were chosen in this model.

**Answer:** 


```{r}

model_lasso_cv <- cv.glmnet(as.matrix(covariates),response, alpha = 1,
                   family = "gaussian")
model_lasso_cv$lambda.min
plot(model_lasso_cv)
coef_cv <- coef(model_lasso_cv, s="lambda.min")


```

```{r}

index <- coef_cv@i
variables <- coef_cv@Dimnames[[1]][index+1]
value <- coef_cv@x

df <- data.frame("Variables" = variables, "Coefficient" = value)

knitr::kable(df, row.names = FALSE,
             caption = "Variables that were chosen in this LASSO model",digits = 3)

```



**Question:**

Does the information displayed in the plot suggests that the optimal $\lambda$ value results in a statistically significantly better prediction than log $\lambda$ = -4?


**Answer:** 

no



**Question:**

Finally, create a scatter plot of the original test versus predicted test values for the model corresponding to optimal lambda and comment whether the model predictions are good.



**Answer:** 

```{r}

## WARNING, NEED TO UNSCALE SOMEHOW ?????????


library(ggplot2)

y <- tecator_test[,"Fat"]
ynew <- predict(model_lasso_cv, newx = as.matrix(tecator_test[, 2:101]),
                type = "response")


ggplot(data.frame(y,ynew), aes(y, lambda.1se)) + geom_point() + theme_bw() +
  labs(x = "True Y",
       y = "Predicted Y by LASSO model")


```








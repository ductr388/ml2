---
title: "Computer lab 2 block 1"
author:
- Simge Cinar
- Duc Tran
- William Wiik
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
    bookdown::pdf_document2: default
    fig_caption: yes
    number_sections: yes
geometry: top=100pt,bottom=100pt,left=68pt,right=66pt
subtitle: 732A99
header-includes:
- \usepackage{booktabs}
- \usepackage{float}
- \usepackage{longtable}
- \usepackage{caption}
- \usepackage{fancyhdr}
- \usepackage{titling}
- \usepackage[swedish, english]{babel}
- \renewcommand{\headrulewidth}{0pt}
- \renewcommand{\and}{\\}
- \pretitle{\centering\vspace{0cm}{\large Laboration report in Machine Learning
  \par}\vspace{4cm}\Huge\textbf}
- \posttitle{\vspace{1cm}\large\textbf{}\par}
- \preauthor{\centering\vspace{4cm}\normalsize}
- \postauthor{\par\vspace{3cm}}
- \predate{\centering{\normalsize Division of Statistics and Machine Learning \\ Department
  of Computer Science \\ Linköping University \par}}
- \postdate{\par\vspace{2cm}}
- \raggedbottom
---

<!-- <!-- Väljer språk till svenska för automatiska titlar -->
<!-- \selectlanguage{swedish} -->

<!-- Byter språket på figur- och tabellbeskrivningar till angivna namn -->
\captionsetup[table]{name = Table}


<!-- Anger sidnumreringens position -->
\fancyhf{}
\fancyfoot[C]{\thepage}
\pagestyle{fancy}

<!-- Tar bort sidnumrering för förteckningar och titelsidan -->
\pagenumbering{gobble}

<!-- Anger sidbrytning -->
\clearpage

<!-- Skapar en innehållsförteckning och anger djupet av rubrikerna som ska visas -->
\setcounter{tocdepth}{3}

<!-- Anger sidbrytning -->
\clearpage

<!-- Börjar sidnumreringen på sida 1 efter att alla förteckningar visats -->
\pagenumbering{arabic}
\setcounter{page}{1}

<!-- Börjar med kapitel 1 -->

```{r options, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
library(tree)
library(knitr)
library(dplyr)
library(kableExtra)
knitr::opts_chunk$set(
  echo = TRUE, 
  fig.width = 4.5, 
  fig.height = 3)
```



# Assignment 2. Decision trees and logistic regression for bank marketing.

The data in this assignment is related with direct marketing campaigns of a 
Portuguese banking institution. Data consists of 21 variables, 20 are input variables
about the clients and the output variable is if the client has a term deposit 
(binary variable).

## 2.1

**Question:** Import the data to R, **remove variable "duration"** and divide into
training/validation/test as 40/30/30: use data partitioning code specified in
Lecture 2a.  

**Answer:**
Data is read into R, where all character variables are made into factor variables,
and the variable duration was removed. 

```{r}
# Index 12 is the variable duration
data <- read.csv2("bank-full.csv", stringsAsFactors = TRUE)[, -12]

# Data partitioning (40/30/30)
# Training data
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.4))
train=data[id,]

# Validation data
id1=setdiff(1:n, id)
set.seed(12345)
id2=sample(id1, floor(n*0.3))
valid=data[id2,]

# Test data
id3=setdiff(id1,id2)
test=data[id3,] 
```

After the splitting the number of observations in the data sets are as follows:

* Training: 18 084 observations.
* Validation: 13 563 observations.
* Test: 13 563 observations.

\clearpage

## 2.2

**Question:** Fit decision trees to the training data so that you change the 
default settings one by one (i.e. not simultaneously):  
  a. Decision Tree with default settings.  
  b. Decision Tree with smallest allowed node size equal to 7000.  
  c. Decision trees minimum deviance to 0.0005.  
and report the misclassification rates for the training and validation data.
Which model is the best one among these three? Report how changing the
deviance and node size affected the size of the trees and explain why.

**Answer:**

The default setting in tree for smallest allowed node size is 10 and the default
for minimum deviance is 0.01. The three different trees were fitted and training error, validation error and number of leaves are presented in table \@ref(tab:tree).

```{r}
# Tree a: default setting
tree_a <- tree(y ~., data=train)
# Misclassification for training data for tree a
pred_train <- predict(tree_a, newdata=train, type="class")
table_train <- table(train$y, pred_train)
miss_train <- 1 - (sum(diag(table_train)) / sum(table_train))
# Misclassification for validation data for tree a
pred_valid <- predict(tree_a, newdata=valid, type="class")
table_valid <- table(valid$y, pred_valid)
miss_valid <- 1 - (sum(diag(table_valid)) / sum(table_valid))
# Number of leaves
leaf <- sum(tree_a$frame$var == "<leaf>")
# Misclassification for tree a
model_a <- c(miss_train, miss_valid, leaf)



# Tree b: smallest size changed from 10 (default) to 7 000
tree_b <- tree(y ~., data=train, control=tree.control(nobs = nrow(train),
                                                      minsize = 7000))
# Misclassification for training data for tree b
pred_train <- predict(tree_b, newdata=train, type="class")
table_train <- table(train$y, pred_train)
miss_train <- 1 - (sum(diag(table_train)) / sum(table_train))
# Misclassification for validation data for tree b
pred_valid <- predict(tree_b, newdata=valid, type="class")
table_valid <- table(valid$y, pred_valid)
miss_valid <- 1 - (sum(diag(table_valid)) / sum(table_valid))
# Number of leaves
leaf <- sum(tree_b$frame$var == "<leaf>")
# Misclassification for tree b
model_b <- c(miss_train, miss_valid, leaf)


# Tree c: mindev changed from 0.01 (default) to 0.0005
tree_c <- tree(y ~., data=train, control=tree.control(nobs = nrow(train),
                                                      mindev = 0.0005))
# Misclassification for training data for tree c
pred_train <- predict(tree_c, newdata=train, type="class")
table_train <- table(train$y, pred_train)
miss_train <- 1 - (sum(diag(table_train)) / sum(table_train))
# Misclassification for validation data for tree c
pred_valid <- predict(tree_c, newdata=valid, type="class")
table_valid <- table(valid$y, pred_valid)
miss_valid <- 1 - (sum(diag(table_valid)) / sum(table_valid))
# Number of leaves
leaf <- sum(tree_c$frame$var == "<leaf>")
# Misclassification for tree c
model_c <- c(miss_train, miss_valid, leaf)
```


```{r tree, echo=FALSE, fig.cap = "\\label{}Decision tree fitted with default settings."}
# Summarised results
table <- data.frame(rbind(model_a, model_b, model_c))
colnames(table) <- c("Training error", "Validation error", "Number of leaves")
rownames(table) <- c("Tree a: default setting", "Tree b: smallest node 7000", 
                     "Tree c: min deviance 0.0005")

kable(table, digits=6, booktabs=T,
      caption="Misclassification error for the three trees on training and validation data.") %>%
   kable_styling(latex_options = "HOLD_position")
```

From table \@ref(tab:tree), the training error and validation error for tree a 
and b are the same, however there is a difference in number of leaves where tree 
a has one more leaf. In tree b, the minimum node size is changed to 7 000 from 10
which forces the tree to have least 7 000 observations in each node. The 
increase in minimum node size makes that the tree can not split nodes smaller 
than 7 000 and thus the tree will have less leaves. 

Training error is lowest for tree c, but the validation error
is also highest for tree c. This indicates that tree c is more overfitted on 
training data compared to the tree a and b. The difference for tree c is that 
minimum deviance is changed from 0.01 to 0.0005. Minimum deviance is how much 
within-node deviance a node have to have compared to the root node to be able to
split. By having a smaller number, the tree is allowed to split nodes with smaller 
deviance, which in turns allows the tree to grow and have smaller nodes than before.

Between tree a, b, and c the tree b is best since it has lowest validation error
but also has the least amounts of leaves. By having less leaves the model is less
complex and is easier to interpret.

\clearpage

## 2.3

**Question:** Use training and validation sets to choose the optimal tree depth 
in the model 2c: study the trees up to 50 leaves. Present a graph of the dependence
of deviances for the training and the validation data on the number of leaves
and interpret this graph in terms of bias-variance tradeoff. Report the
optimal amount of leaves and which variables seem to be most important for
decision making in this tree. Interpret the information provided by the tree
structure (not everything but most important findings).

**Answer:**
Tree c is used where the tree is post-pruned to trees with between 2 and 50 leaves.
The deviance for training data and validation data are calculated and the code is
as follows:


```{r}
fit <- tree(y ~., data=train, 
            control=tree.control(nobs = nrow(train),
                                 mindev = 0.0005))
trainScore=rep(0,50)
testScore=rep(0,50)
for(i in 2:50){
  prunedTree=prune.tree(fit,best=i)
  pred=predict(prunedTree, newdata=valid,
               type="tree")
  trainScore[i]=deviance(prunedTree)
  testScore[i]=deviance(pred)
}
```

The tree with the lowest validation error had 22 leaves.
```{r}
# Finds min, add +1 since index 1 is tree with 2 leaves.
which(min(testScore[2:50]) == testScore[2:50])+1
```

The dependence of deviances for training and validation data for different numbers
of leaves is presented in figure \@ref(dependencePlot). 

\clearpage


```{r, echo=FALSE, fig.height=3, fig.width=5, fig.cap = "\\label{dependencePlot}Dependence of deviances for training- and testdata."}
plot_data <- data.frame(training = trainScore[-1], test = testScore[-1])
ggplot(plot_data, aes(x=2:50)) +
  geom_point(aes(y=training, color="training")) +
  geom_point(aes(y=test, color="test")) +
  geom_vline(xintercept=22, linetype="dashed") +
  ylim(6000, 12000) +
  labs(x="Number of leaves", y="Deviance") +
  scale_colour_manual(name="Data", 
                      values=c("indianred", "steelblue"),
                      labels=c("Test", "Training")) +
  theme_bw()

# 
# plot(2:50, trainScore[2:50], type="b", col="red",ylim=c(8000,12000))
# points(2:50, testScore[2:50], type="b", col="blue")
# # Fill the point with the min value on validation
# points(22, testScore[22], pch=16, col="blue")
```

In figure \@ref(dependencePlot) the dashed line shows where the tree with the lowest
deviance for testdata is located. For tree models, the complexity increases when the
number of leaves increase. In terms of bias-variance tradeoff a model with low 
complexity (few leaves) have high bias but low variance and is considered to be 
underfitting. In contrast a model with high complexity have low bias but high variance
and is considered to be overfitting. In figure \@ref(dependencePlot) the tree 
with 22 leaves (dashed line) is considered to be the tree where we have a 
balance between low/high values for bias and variance which gives us that the 
tree has the optimal fit since it is not underfitting or overfitting.

The tree with 22 leaves is considered to be optimal and is presented in figure \@ref(bestTree).

\clearpage

```{r, fig.height=8, fig.width=10, fig.cap = "\\label{bestTree} Tree with 22 leaves."}
best_fit <- prune.tree(fit, best=22)
plot(best_fit)
text(best_fit, pretty=0)
```

In figure \@ref(bestTree) most of the leaves have the prediction "no". The leaves
with the predictions "yes" used the variables: poutcome, month, contact, pdays,
and job. These 5 variables seems to be the most important for decision making for 
this tree. The tree structure is presented in the following output:

```{r}
best_fit
```


From the output the four leaves that predicts "yes" have the probabilities between
0.58969 and 0.76471.  
**What more should we comment?** 
\clearpage

## 2.4

**Question:** Estimate the confusion matrix, accuracy and F1 score for the test data by
using the optimal model from step 3. Comment whether the model has a
good predictive power and which of the measures (accuracy or F1-score)
should be preferred here. 


<<<<<<< HEAD
**Answer:** The confusion matrix is estimated for the test data and is presented
in the following output:


```{r, confMatTree}
# Confusion matrix
pred_test <- predict(best_fit, newdata=test, type="class")
conf_mat_tree <- table(True = test$y, Predicton = pred_test)
conf_mat_tree

```

The accuracy of the model is around 89%.

```{r}
# Accuracy
acc_test <- sum(diag(conf_mat_tree)) / sum(conf_mat_tree)
acc_test
```

The F1 score is around 0.22.
=======
**Answer:** The confusion matrix for the test data is presented in \@ref(tab:conf_mat).

```{r conf_mat}
# Prediction on test data
pred_test <- predict(best_fit, newdata=test, type="class")
table_test <- table(True=test$y, Predict=pred_test)
# Confusion matrix
table_test
```

From \@ref(tab:conf_mat) we can see that we have unbalanced data where "no" is the
dominant class. The model is therefore better at prediction "no" compared to yes.
The accuracy of the model is around 89.10%

```{r}
acc_test <- sum(diag(table_test)) / sum(table_test)
acc_test
```

The F1-score the data is around 0.2246.
>>>>>>> b8b6b4be5c1198f28947f72fdee8bb71a6e16b9e

```{r}
# F1-score
# True positive
<<<<<<< HEAD
tp <- conf_mat_tree[2,2]
# False positive
fp <- conf_mat_tree[1,2]
# False negative
fn <- conf_mat_tree[2,1]

precision <- tp / (tp+fp)
recall <- tp / (tp+fn)
# F-score is between 0 and 1, where the closer to 1, the better. 
f1_score <- 2 * (precision*recall) / (precision + recall)
f1_score
```


The accuracy of the model is around 89%, which looks high. But examining "yes",
the model only predicted around 13.5% (214/(214+1371)) of these observations correctly. 
Confusion matrix indicate that we have unbalanced data, which makes the
model able to predict no (the dominant class) better than yes. 
F1-score focuses more on the prediction "yes" compared to accuracy. In this case
we have really unbalanced classes and F1-score is therefore a better measurement.
F1-score is between 0 and 1, where the closer F1-score is to 1 the better. The model
has a F1-score of 0.22 and is not considered to have good predictive power.


\clearpage
=======
TP <- table_test[2,2]
# false positve = We predict positive(yes) but real value is negative(no).
FP <- table_test[1,2]
# false negative = We predict negative(no) but real value is positive(yes)
FN <- table_test[2,1]

precision <- TP / (TP+FP)
recall <- TP / (TP+FN)
# F-score is between 0 and 1, where the closer to 1, the better. 
f_score <- 2 * (precision*recall) / (precision + recall)
f_score
```

>>>>>>> b8b6b4be5c1198f28947f72fdee8bb71a6e16b9e

## 2.5

**Question:** Perform a decision tree classification of the test data with the following loss matrix, 

<div class="math">
\[
L = _{Observed}
\begin{array}{cc} &
\begin{array}{cc} Predicted \end{array}
\\
\begin{array}{cc}
yes \\
no \end{array}
&
\left(
\begin{array}{cc}
0 & 5 \\
1 & 0 \end{array}
\right)\end{array}
\]
</div>
and report the confusion matrix for the test data. Compare the results with
the results from step 4 and discuss how the rates has changed and why.

**Answer:**

## 2.6

**Question:**
**Answer:**



\clearpage

# Statement of Contribution
We worked on the assignment individually for the computer labs (to be more 
efficient when asking questions), William on task 1, Duc on task 2, and Simge on task 3.
We later solved all assignment individually and compared and discussed our solutions
before dividing the task of writing the laboration report. 


## Question 1

Text written by William. 

## Question 2

Text written by Duc.

## Question 3

Text written by Simge. 


# Appendix 
The code used in this laboration report are summarised in the code as follows:

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```





